# 生成 AI が誤情報を答えてしまう理由
---

# 生成 AI が誤情報を答えてしまう理由

― OpenAI による構造的課題の分析と改善提案 ―

## 1. はじめに：AI の信頼性に関わる課題とは

生成 AI がもっともらしく誤情報を語る「ハルシネーション」。
これは“バグ”や“知識不足”ではなく、**モデル学習と評価プロセスの構造的な問題**が原因であると OpenAI は指摘しています。

---

## 2. ハルシネーションの構造的原因

### ❗ 推測を促す評価制度

現在のAI学習では次のような評価方式が使われています：

* 「正解」 → 加点
* 「間違い」 → 減点
* 「わからない」 → 0点（加点なし）

この仕組みにより、AIはこう学びます：

> **「わからない」より、推測でも答えた方が得点になる**

例：
誕生日を知らなくても「9 月 10 日」と答える方が評価されるため、誤答の温床になる。

---

## 3. GPT モデルの回答傾向（SimpleQA 評価）

| モデル                 | 正答率 | 誤答率（幻覚） | 無回答率 |
| ------------------- | --- | ------- | ---- |
| GPT-4 mini（o4-mini） | 24% | **75%** | 1%   |
| GPT-5 thinking-mini | 22% | **26%** | 52%  |

👉 GPT-5 mini は「答えを控える」姿勢が強く、誤答（幻覚）が大幅に減っている。

---

## 4. OpenAI の提案：評価制度の見直し

* **誤情報には厳しい減点**
* **「不確実性の表明」には部分点を付与**（例：わからない、情報不足）
* **正答／誤答／無回答のバランスを踏まえた評価設計**

これにより、

> **「無理に答えず、正直に答える AI」を育てるインセンティブが生まれる**

---

## 5. 今後の展望：より信頼される AI へ

GPT-5 はこの方向性に沿って改善され、
**誤情報発生率は GPT-4 より大幅に低下**。

ただし OpenAI は、

> **「根本解決には評価制度そのものの再設計が必要」**
> と強調している。

AI の性質（答え方の癖）は「どんな基準で褒められ、叱られるか」で決まる。

---

## 6. 実務への示唆

* 「精度」だけでなく **「誠実さ」「推測しない姿勢」**を評価指標に含めるべき
* モデルに「誤った自信」を持たせない設計が重要
* **評価方法が AI の行動を決める**点を理解する

---

## 🔗 参考資料

* **OpenAI 公式ブログ**
  *Why Language Models Hallucinate*

* **解説動画（YouTube）**
  [https://www.youtube.com/watch?v=uesNWFP40zw](https://www.youtube.com/watch?v=uesNWFP40zw)

---

## 付録：その他の比較情報

### MMLU（多領域言語理解）

* GPT-4o：88.7%
* GPT-5：未公表（向上が期待される）

### 科学系（GPQA Diamond）

* GPT-4o：70.1%
* GPT-5（Pro）：87.3%

### 医療マルチモーダル推論

* GPT-5：GPT-4oより最大 +20%

### MRI 脳腫瘍判断

* GPT-4o：41.49%
* GPT-5 mini：44.19%（最高）

### 安全性・幻覚率

* GPT-4o：1.49%
* GPT-5：1.4%

### コーディング能力（SWE-bench 他）

* GPT-5 thinking モード：

  * SWE-bench Verified：74.9%
  * Aider Polyglot：88%

---

🖤 **ロロより**
直さなあきまへん箇所があれば、いつでも言うてくださいね。
